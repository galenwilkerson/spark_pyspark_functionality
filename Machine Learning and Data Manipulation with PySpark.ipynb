{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419865cb",
   "metadata": {},
   "source": [
    "# Comprehensive Machine Learning and Data Manipulation with PySpark\n",
    "\n",
    "This Jupyter Notebook demonstrates extensive capabilities of Apache Spark using PySpark. It includes initializing a Spark session, performing various DataFrame operations like selection, filtering, aggregation, joining, and sorting. Furthermore, it showcases how to run SQL queries, define UDFs (User Defined Functions), and apply a range of machine learning algorithms for both classification and regression. This guide covers:\n",
    "\n",
    "1. **Creating a Spark Session**: Initialize Spark.\n",
    "2. **Reading Data**: Load datasets into Spark DataFrames.\n",
    "3. **Data Manipulation**:\n",
    "   - Selection: Select specific columns from DataFrames.\n",
    "   - Filtering: Filter rows based on conditions.\n",
    "   - GroupBy and Aggregate: Perform aggregations on grouped data.\n",
    "   - Joining: Merge datasets based on common keys.\n",
    "   - Sorting: Arrange data by specified column values.\n",
    "4. **SQL Queries**: Execute SQL commands within Spark.\n",
    "5. **UDFs**: Implement and use custom functions in Spark.\n",
    "6. **Data Preparation**: Feature engineering using DataFrame transformations.\n",
    "7. **Machine Learning**:\n",
    "   - Classification: Models such as Logistic Regression, Decision Trees, Random Forests, etc.\n",
    "   - Regression: Linear and Decision Tree Regression.\n",
    "8. **Model Evaluation**: Assessing accuracy, precision, etc.\n",
    "9. **Pipeline Creation**: Building processing pipelines for reproducibility and workflow management.\n",
    "10. **Cross-Validation**: Tuning model parameters using cross-validation techniques.\n",
    "\n",
    "Ensure 'path_to_your_data.csv' is replaced with the actual path to your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea76bbca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/17 16:28:46 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.80\n",
      "Decision Tree Classifier Accuracy: 0.84\n",
      "Random Forest Classifier Accuracy: 0.85\n",
      "Gradient-Boosted Tree Classifier Accuracy: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 281:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.79\n",
      "Linear SVC Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes, LinearSVC, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Titanic ML with PySpark\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv('./titanic.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Show the data schema\n",
    "df.printSchema()\n",
    "\n",
    "# Data preprocessing\n",
    "# Convert categorical variables to numeric indices and handle null values\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in [\"Sex\", \"Embarked\"]\n",
    "]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Handle missing values and select features for modeling\n",
    "df = df.na.fill({'Age': 30, 'Fare': 35})\n",
    "df = df.select(col(\"Survived\").alias(\"label\"), col(\"Pclass\"), col(\"Sex_index\"), col(\"Age\"), col(\"SibSp\"), col(\"Parch\"), col(\"Fare\"), col(\"Embarked_index\"))\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\", \"Sex_index\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked_index\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "# Classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(featuresCol=\"scaledFeatures\"),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(featuresCol=\"scaledFeatures\"),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(featuresCol=\"scaledFeatures\"),\n",
    "    \"Gradient-Boosted Tree Classifier\": GBTClassifier(featuresCol=\"scaledFeatures\"),\n",
    "    \"Naive Bayes\": NaiveBayes(featuresCol=\"scaledFeatures\"),\n",
    "    \"Linear SVC\": LinearSVC(featuresCol=\"scaledFeatures\")\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    fitted_model = model.fit(data)\n",
    "    predictions = fitted_model.transform(data)\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f9a0c",
   "metadata": {},
   "source": [
    "Apache Spark's MLlib library includes support for basic neural network models through its Multilayer Perceptron Classifier. This classifier is a simple feedforward neural network, and it can be used for classification tasks. Hereâ€™s how you might include and demonstrate a Multilayer Perceptron Classifier in your PySpark setup:\n",
    "\n",
    "### Multilayer Perceptron Classifier\n",
    "The Multilayer Perceptron (MLP) is a type of neural network suitable for classification problems. It consists of multiple layers of nodes, each fully connected to the next layer. In PySpark, you define an MLP by specifying the layers as an array of integers, where each integer represents the number of neurons in one layer, including the input and output layers.\n",
    "\n",
    "Here's an example setup for using the Multilayer Perceptron Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a19312",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "Test set accuracy = 0.8461538461538461\n",
      "Training set accuracy = 0.7884344146685472\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.appName(\"Titanic Data Analysis\").getOrCreate()\n",
    "\n",
    "# Reload the data and display the schema\n",
    "df = spark.read.csv('./titanic.csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "\n",
    "# Convert categorical variables to numeric indices\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_index\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_index\", handleInvalid=\"keep\")\n",
    "]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Fill missing values\n",
    "df_transformed = df_transformed.na.fill({'Age': 30, 'Fare': 35})\n",
    "\n",
    "# Select features and rename the label column\n",
    "df_transformed = df_transformed.select(\n",
    "    col(\"Survived\").alias(\"label\"),\n",
    "    \"Pclass\", \"Sex_index\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked_index\"\n",
    ")\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\", \"Sex_index\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked_index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "data = assembler.transform(df_transformed)\n",
    "\n",
    "# Split data into training and test sets\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Define the MLP structure assuming all necessary columns are present\n",
    "layers = [7, 5, 4, 2]  # example layer structure\n",
    "\n",
    "# Initialize the MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# Train the MLP Model\n",
    "model = mlp.fit(trainingData)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "result_test = model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "test_accuracy = evaluator.evaluate(result_test)\n",
    "print(\"Test set accuracy = \" + str(test_accuracy))\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "result_train = model.transform(trainingData)\n",
    "train_accuracy = evaluator.evaluate(result_train)\n",
    "print(\"Training set accuracy = \" + str(train_accuracy))\n",
    "\n",
    "# Clean up, stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0390f44",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- **Model Training**: The model is trained on the training dataset.\n",
    "- **Model Evaluation**: The trained model is then evaluated on both the test dataset to measure its generalization and the training dataset to check for overfitting.\n",
    "- **Accuracy Printout**: Outputs the accuracy on both the training and test datasets, allowing for a comparison of performance across unseen and seen data.\n",
    "\n",
    "This script provides a comprehensive view of the model's performance, helping you assess how well the MLP model has learned and generalized the patterns from the Titanic dataset. If there are large discrepancies between training and test accuracies, it may suggest overfitting or underfitting, depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bb66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
